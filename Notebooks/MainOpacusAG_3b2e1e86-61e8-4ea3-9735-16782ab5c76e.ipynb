{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Basic Machine Learning Model with AG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is all about building, training, and evaluating a machine learning model with differential privacy using the Iris dataset in Antigranular. Let's dive into the world of flowers and privacy-preserving ML! ü¶æ\n",
    "\n",
    "The Iris dataset is super famous in the ML world. It's often used for classification tasks and contains samples of iris flowers described by four features: sepal length, sepal width, petal length, and petal width. üå∏üåº\n",
    "\n",
    "By the end of this notebook, you'll be able to:\n",
    "\n",
    "1. Load and preprocess data.\n",
    "2. Build and train a neural network model with differential privacy.\n",
    "3. Evaluate the model's performance.\n",
    "4. Save and reload the trained model for future use.\n",
    "\n",
    "Let's get started with loading and analysing the data. üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing and Loading Configuration\n",
    "\n",
    "First, let's get everything set up. We need to install and import the necessary libraries and then log in to our Antigranular session. üõ†Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: antigranular in /usr/local/lib/python3.10/dist-packages (0.3.0)\n",
      "Requirement already satisfied: ipython<8.0.0,>=7.34.0 in /usr/local/lib/python3.10/dist-packages (from antigranular) (7.34.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from antigranular) (1.25.2)\n",
      "Requirement already satisfied: oblv-client<0.2.0,>=0.1.15 in /usr/local/lib/python3.10/dist-packages (from antigranular) (0.1.15)\n",
      "Requirement already satisfied: onnx<2.0.0,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from antigranular) (1.16.1)\n",
      "Requirement already satisfied: pandas<2.0.0,>=1.5.3 in /usr/local/lib/python3.10/dist-packages (from antigranular) (1.5.3)\n",
      "Requirement already satisfied: pydantic<2.0.0,>=1.10.7 in /usr/local/lib/python3.10/dist-packages (from antigranular) (1.10.15)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from antigranular) (2.31.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython<8.0.0,>=7.34.0->antigranular) (67.7.2)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython<8.0.0,>=7.34.0->antigranular) (0.19.1)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython<8.0.0,>=7.34.0->antigranular) (4.4.2)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython<8.0.0,>=7.34.0->antigranular) (0.7.5)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython<8.0.0,>=7.34.0->antigranular) (5.7.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython<8.0.0,>=7.34.0->antigranular) (3.0.45)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython<8.0.0,>=7.34.0->antigranular) (2.16.1)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython<8.0.0,>=7.34.0->antigranular) (0.2.0)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython<8.0.0,>=7.34.0->antigranular) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython<8.0.0,>=7.34.0->antigranular) (4.9.0)\n",
      "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from oblv-client<0.2.0,>=0.1.15->antigranular) (2.0.7)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx<2.0.0,>=1.15.0->antigranular) (3.20.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.0.0,>=1.5.3->antigranular) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.0.0,>=1.5.3->antigranular) (2023.4)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.0.0,>=1.10.7->antigranular) (4.12.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->antigranular) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->antigranular) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->antigranular) (2024.2.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython<8.0.0,>=7.34.0->antigranular) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython<8.0.0,>=7.34.0->antigranular) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<8.0.0,>=7.34.0->antigranular) (0.2.13)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas<2.0.0,>=1.5.3->antigranular) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install antigranular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local_host_port: ba89e8bb-8683-4f75-bfe1-a2caeb83cef0\n",
      "server_hostname: ip-100-100-17-79.eu-west-1.compute.internal\n",
      "tls_cert_name: ip-100-100-17-79.eu-west-1.compute.internal_ba89e8bb-8683-4f75-bfe1-a2caeb83cef0\n",
      "Dataset \"Medical Treatment\" loaded to the kernel as \u001b[92mmedical_treatment\u001b[0m\n",
      "Key Name                       Value Type     \n",
      "---------------------------------------------\n",
      "train_x                        PrivateDataFrame\n",
      "train_y                        PrivateDataFrame\n",
      "test_x                         DataFrame      \n",
      "\n",
      "Connected to Antigranular server session id: 3b2e1e86-61e8-4ea3-9735-16782ab5c76e, the session will time out if idle for 25 minutes\n",
      "Cell magic '%%ag' registered successfully, use `%%ag` in a notebook cell to execute your python code on Antigranular private python server\n",
      "üöÄ Everything's set up and ready to roll!\n"
     ]
    }
   ],
   "source": [
    "import antigranular as ag\n",
    "\n",
    "session = ag.login(\n",
    "    \"8sqSztw0PjQvNWR4JTQktnbtGO5KmR3t\",\n",
    "    \"M9BJrFVyo0mryGakX4xnYWRyXKtNvg16pV3rp3nZzfcxPD0CunbFJZSzE_mTDznZ\",\n",
    "    competition=\"Sandbox Competition\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data + Basic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we're going to load the Iris dataset and do some basic analysis to get a feel for the data. Privacy is our priority, so we'll make sure to use a PrivateDataFrame. üîí Think of this as our first peek into the world of irises, but with a privacy-preserving twist. üòâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframe cached to server, loading to kernel...\n",
      "DataFrame loaded successfully to the kernel\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "URL = \"https://content.antigranular.com/image/notebook_content/Iris.csv\"\n",
    "\n",
    "iris_dataset = pd.read_csv(URL)\n",
    "\n",
    "session.private_import(iris_dataset, \"iris_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ag\n",
    "from op_pandas import PrivateDataFrame\n",
    "\n",
    "data = PrivateDataFrame(iris_dataset, metadata = {'PetalLengthCm': (1.0, 7.0), 'PetalWidthCm': (0.0, 2.5), 'SepalLengthCm': (4.0, 8.0), 'SepalWidthCm': (2.0, 5.0)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `info()` method provides a concise summary of the DataFrame, including column names, data types, and non-null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------+-------------+---------------+---------+------------+\n",
      "|    | Column        | numerical   | categorical   | dtype   | bounds     |\n",
      "|----+---------------+-------------+---------------+---------+------------|\n",
      "|  0 | Id            | True        | False         | int64   | None       |\n",
      "|  1 | SepalLengthCm | True        | False         | float64 | (4.0, 8.0) |\n",
      "|  2 | SepalWidthCm  | True        | False         | float64 | (2.0, 5.0) |\n",
      "|  3 | PetalLengthCm | True        | False         | float64 | (1.0, 7.0) |\n",
      "|  4 | PetalWidthCm  | True        | False         | float64 | (0.0, 2.5) |\n",
      "|  5 | Species       | False       | False         | object  | None       |\n",
      "+----+---------------+-------------+---------------+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%ag\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the `metadata` which provides the bounds for each numerical column‚Äîsuper important for ensuring data privacy. This metadata is like our guide, ensuring we don't accidentally peek too much. üîç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PetalLengthCm': (1.0, 7.0), 'PetalWidthCm': (0.0, 2.5), 'SepalLengthCm': (4.0, 8.0), 'SepalWidthCm': (2.0, 5.0)}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%ag\n",
    "\n",
    "ag_print(data.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The `describe()` method shows descriptive statistics like mean, standard deviation, minimum, and maximum values for each numerical feature. These stats give us a snapshot of our data's central tendencies and spread. üìä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm\n",
      "count  150.000000     212.000000    147.000000     147.000000    161.000000\n",
      "mean    75.500000       5.620798      2.000000       3.718233      1.007954\n",
      "std     43.445368       1.668323      0.142438       2.896114      0.296229\n",
      "min      1.000000       4.000000      2.000000       1.000000      0.000000\n",
      "25%     38.250000       5.168620      2.397140       2.929862      0.105924\n",
      "50%     75.500000       6.084685      2.658683       4.660259      1.048370\n",
      "75%    112.750000       6.768751      2.996746       4.657727      0.111331\n",
      "max    150.000000       6.349513      4.242385       4.486505      2.498177\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%ag\n",
    "\n",
    "ag_print(data.describe(eps = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation matrix shows the relationships between different numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              PetalLengthCm PetalWidthCm SepalLengthCm SepalWidthCm\n",
      "PetalLengthCm           1.0     0.846679      1.106034     0.522887\n",
      "PetalWidthCm       0.846679          1.0      0.767682    -0.145475\n",
      "SepalLengthCm      1.106034     0.767682           1.0     0.092484\n",
      "SepalWidthCm       0.522887    -0.145475      0.092484          1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%ag\n",
    "\n",
    "ag_print(data.corr(eps = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Iris dataset, we see high positive correlations between `PetalLengthCm` and `PetalWidthCm`, indicating that as the length of the petals increases, the width also tends to increase.  üå±"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing for Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to prep our data for training! üëäüèº In this section, we will preprocess the data to prepare it for training a machine learning model. Preprocessing includes importing necessary libraries, splitting the data into training and test sets, selecting features, encoding the target variables, and normalizing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Necessary Libraries üìö\n",
    "First, let's bring in all the tools we'll need for this ML journey. üß∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ag\n",
    "import pandas as pd\n",
    "import op_pandas as opd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "from op_opacus import PrivateDPDataLoader, PrivacyEngine, ApplyModel, TrainModel, make_loss_private\n",
    "from op_pandas import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using libraries like `pandas` and `numpy` for data manipulation. `torch` is used for building and training the neural network model. `opacus` provides tools to ensure differential privacy during model training, and `op_pandas` offers functions tailored for differentially private data handling. üïµüèª‚Äç‚ôÄÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Data into Train and Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll split our data into training and test sets to see how our model performs on new, unseen data. This is like dividing your study material before an exam‚Äîtrain on one part, test on the other. ‚úÇÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ag\n",
    "train, test = train_test_split(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Id', 'SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm', 'Species']\n",
      "['Id', 'SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm', 'Species']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%ag\n",
    "ag_print(train.columns)\n",
    "ag_print(test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's select the features (input variables) and the target variable for our model. This step involves choosing which columns will be used as inputs for training and which column will be the target we want to predict. üëÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ag\n",
    "train_x = train[[\"PetalLengthCm\", \"PetalWidthCm\", \"SepalLengthCm\", \"SepalWidthCm\"]]\n",
    "train_y = train[\"Species\"]\n",
    "test_x = test[[\"PetalLengthCm\", \"PetalWidthCm\", \"SepalLengthCm\", \"SepalWidthCm\"]]\n",
    "test_y = test[\"Species\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** We select four features: `PetalLengthCm`, `PetalWidthCm`, `SepalLengthCm`, and `SepalWidthCm`. These features will be used to train the model. The target variable is the species of the iris flowers, which we aim to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       PetalLengthCm  PetalWidthCm  SepalLengthCm  SepalWidthCm\n",
      "count     140.000000     48.000000     146.000000    145.000000\n",
      "mean        4.693648      1.154522       5.284139      2.927259\n",
      "std         2.652672      0.844226       0.688012      0.309831\n",
      "min         1.000000      0.000000       4.000000      2.000000\n",
      "25%         3.706785      2.407373       5.539526      2.445628\n",
      "50%         1.223581      0.735186       4.705366      3.048147\n",
      "75%         3.491770      1.467781       4.846437      3.710183\n",
      "max         6.696962      2.304352       4.371945      2.033162\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%ag\n",
    "ag_print(train_x.describe(eps=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the Train / Test Outputs Using map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encode the categorical target variable (species) into numerical values to facilitate model training. Most machine learning algorithms, including neural networks, require numerical input. Think of this step as translating the species names into numbers that our model can understand. üî¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ag\n",
    "\n",
    "def func_(x:str)-> int:\n",
    "    if x == 'Iris-setosa':\n",
    "        return 0\n",
    "    elif x == 'Iris-versicolor':\n",
    "        return 1\n",
    "    else: # Iris-virginica\n",
    "        return 2\n",
    "\n",
    "train_y_encoded = train_y.map(func_ , output_bounds=(0,2))\n",
    "test_y_encoded = test_y.map(func_, output_bounds=(0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ag\n",
    "train_y = train_y_encoded\n",
    "test_y = test_y_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 2)\n",
      "(0, 2)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%ag\n",
    "\n",
    "ag_print(train_y.metadata)\n",
    "ag_print(test_y.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Printing the metadata for the encoded target variables helps ensure that the encoding was performed correctly and that the numerical values fall within the expected bounds (0 to 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Data Loader, Neural Network, Optimiser and Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will create the data loader, define the neural network model, set up the optimiser, and define the loss function. These are crucial steps for training a machine learning model. ‚úÖ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ag\n",
    "# data loader from private dataframe\n",
    "data_loader = PrivateDPDataLoader.from_private_dataframe(\n",
    "    [train_x, train_y], dtypes=[torch.float, torch.long]\n",
    ")\n",
    "\n",
    "# sequential model\n",
    "model = nn.Sequential(nn.Linear(4, 16),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(16, 3),\n",
    "                     nn.Softmax())\n",
    "\n",
    "# stochastic gradient descent model\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Cross entropy loss\n",
    "PrivateCrossEntropyLoss = make_loss_private(nn.CrossEntropyLoss)\n",
    "loss_function = PrivateCrossEntropyLoss()   # so that per epoch average loss can be shown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "\n",
    "- First, we'll create the `PrivateDPDataLoader` from our private DataFrame and define the neural network architecture so that it is compatible with training using op_opacus. This network will have an input layer, one hidden layer with ReLU activation, and an output layer with softmax activation. üß†\n",
    "- We use Stochastic Gradient Descent (SGD) as the optimiser with a learning rate of 0.01.\n",
    "- The loss function is cross-entropy loss, made private using `make_loss_private`. It is necessary to do so as it enables op_opacus to calculate average loss per epoch and present it while training. ü•≥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the Privacy Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're setting up the privacy engine to keep our model training private. This step ensures our training process adheres to differential privacy standards. üîíüîß"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/opacus_internal/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/opacus_internal/accountants/analysis/prv/prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%ag\n",
    "privacy_engine = PrivacyEngine()\n",
    "\n",
    "privacy_engine.make_private_with_epsilon(\n",
    "    module=model,\n",
    "    optimizer=optimizer,\n",
    "    data_loader=data_loader,\n",
    "    target_epsilon=3,\n",
    "    target_delta=1e-5,\n",
    "    epochs=10,\n",
    "    max_grad_norm=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "- The PrivacyEngine is used to enforce differential privacy during training.\n",
    "- The `make_private_with_epsilon` method configures the model, optimiser, and data loader with specified privacy budgets (target_epsilon and target_delta), number of epochs, and maximum gradient norm for clipping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ag\n",
    "\n",
    "def train_callable(model, optimizer, z, loss_function):\n",
    "    inputs = z[0]\n",
    "    labels = z[1]\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    loss = loss_function(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The `train_callable` function performs a single training step:\n",
    "\n",
    "It resets gradients, performs a forward pass, computes the loss, performs a backward pass, and updates the model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's get this model trained! We'll use our defined privacy engine, loss function, and training callable. Training is where the magic happens, as our model learns from the data. ‚ú®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed.\n",
      " Epsilon used: 1.4517814813791448,For target delta: 1e-05\n",
      "Average loss for this epoch : 1.1122778595229725, time taken: 1.4373948409920558 seconds.\n",
      "\n",
      "Epoch 2 completed.\n",
      " Epsilon used: 1.7286017897530341,For target delta: 1e-05\n",
      "Average loss for this epoch : 1.0607246393742769, time taken: 1.5032620900019538 seconds.\n",
      "\n",
      "Epoch 3 completed.\n",
      " Epsilon used: 1.9412375107726951,For target delta: 1e-05\n",
      "Average loss for this epoch : 0.9102800861898676, time taken: 1.5879814669897314 seconds.\n",
      "\n",
      "Epoch 4 completed.\n",
      " Epsilon used: 2.1253217178212642,For target delta: 1e-05\n",
      "Average loss for this epoch : 0.9270107269287109, time taken: 1.5768276260059793 seconds.\n",
      "\n",
      "Epoch 5 completed.\n",
      " Epsilon used: 2.2921996452700104,For target delta: 1e-05\n",
      "Average loss for this epoch : 0.8705504393577576, time taken: 1.3092820619931445 seconds.\n",
      "\n",
      "Epoch 6 completed.\n",
      " Epsilon used: 2.447055361402201,For target delta: 1e-05\n",
      "Average loss for this epoch : 0.88616097340217, time taken: 1.4900863580114674 seconds.\n",
      "\n",
      "Epoch 7 completed.\n",
      " Epsilon used: 2.5927681907630062,For target delta: 1e-05\n",
      "Average loss for this epoch : 0.8950180849608254, time taken: 1.2992475900100544 seconds.\n",
      "\n",
      "Epoch 8 completed.\n",
      " Epsilon used: 2.7311477645619333,For target delta: 1e-05\n",
      "Average loss for this epoch : 0.7961993209303242, time taken: 1.397496838006191 seconds.\n",
      "\n",
      "Epoch 9 completed.\n",
      " Epsilon used: 2.863432165624286,For target delta: 1e-05\n",
      "Average loss for this epoch : 0.7338832097394126, time taken: 1.2933977770153433 seconds.\n",
      "\n",
      "Epoch 10 completed.\n",
      " Epsilon used: 2.9905220167459285,For target delta: 1e-05\n",
      "Average loss for this epoch : 0.7651666227508994, time taken: 1.3045530040108133 seconds.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/opacus_internal/accountants/analysis/prv/prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%ag\n",
    "\n",
    "train_model = TrainModel(privacy_engine, loss_function)\n",
    "train_model.train(train_callable, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "\n",
    "- The TrainModel class initialises the training process with the configured privacy engine and loss function.\n",
    "- The train method is called to start the training process, with `verbose=2` to display detailed progress information for each epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the Accuracy of the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to see how well our model performs on the test dataset. This involves applying the model to the test data, decoding the predictions, and calculating the accuracy. This is the moment of truth! ü§ì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/torch/nn/modules/container.py:204: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%ag\n",
    "\n",
    "test_model = ApplyModel(privacy_engine=privacy_engine)\n",
    "output_bounds = {\n",
    "    \"Iris-setosa\": (0, 1),\n",
    "    \"Iris-versicolor\": (0, 1),\n",
    "    \"Iris-virginica\": (0, 1)\n",
    "}\n",
    "\n",
    "out = test_model.apply_model_private(test_x, dtype=torch.float, output_col_names=[\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"], output_bounds = output_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%ag\n",
    "\n",
    "ag_print(out.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "\n",
    "- We use `ApplyModel` from opacus to apply the trained model to the test data while ensuring differential privacy.\n",
    "- The `apply_model_private` method generates predictions for the test data with specified output bounds for each class.\n",
    "- We print the column names of the prediction output to verify the predictions for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoding the Predictions\n",
    "Let's decode the predictions back to their original class labels. This is where we translate the model's \"language\" from numerical values back to the original class labels. üó£Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ag\n",
    "out = out.idxmax()\n",
    "\n",
    "def func_(x: str) -> int:\n",
    "  if x == 'Iris-setosa':\n",
    "      return 0\n",
    "  elif x == 'Iris-versicolor':\n",
    "      return 1\n",
    "  else: # Iris-virginica\n",
    "      return 2\n",
    "\n",
    "out = out.map(func_, output_bounds=(0, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "\n",
    "- The `idxmax` function finds the class with the highest probability for each prediction.\n",
    "- We define a function to map the class labels back to their numerical representations (0 for Iris-setosa, 1 for Iris-versicolor, and 2 for Iris-virginica)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the Accuracy\n",
    "Let's check the accuracy of our predictions. We calculate the accuracy of the model by comparing the predicted values with the actual labels from the test set. This is where we see how well our model has learned to classify the irises. üåºüìà"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7433080849342009\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%ag\n",
    "\n",
    "pred_correctness = (test_y==out)\n",
    "\n",
    "pred_acc = pred_correctness.sum(eps = 1) / pred_correctness.count(eps = 1)\n",
    "\n",
    "ag_print(pred_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "- We compare the predicted values (`out`) with the actual labels (`test_y`) to determine the correctness of each prediction.\n",
    "- The accuracy is calculated as the proportion of correct predictions to the total number of predictions.\n",
    "- We print the accuracy, which represents the percentage of correctly classified instances in the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will export the trained model's state dictionary, which contains all the learned parameters (weights and biases). This allows us to save the model for future use without retraining it. Think of this as saving your game progress so you can pick up right where you left off. üéÆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up exported variable in local environment: state_dict\n"
     ]
    }
   ],
   "source": [
    "%%ag\n",
    "state_dict = model.state_dict()\n",
    "\n",
    "export(state_dict, 'state_dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "\n",
    "- The `state_dict` method retrieves the model's parameters, including weights and biases.\n",
    "- The `export` function is used to save the state dictionary to a file or another environment for future use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section demonstrates how to import the saved state dictionary and load it into a new model instance. This is super handy when you want to use a trained model without retraining it. üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight:\n",
      "[[0.22285522520542145, 0.5076473355293274, -0.7087080478668213, 0.8396198153495789], [-0.8552817702293396, 0.6698233485221863, -0.024328814819455147, -0.34753310680389404], [-0.364187091588974, 0.28911224007606506, 0.5861486792564392, 0.37813711166381836], [-0.3704695701599121, -0.006869448348879814, 0.0845165029168129, 0.4203955829143524], [-0.06879377365112305, 0.35394763946533203, 0.09323772042989731, 0.20437338948249817], [-0.1443086713552475, 0.5684356093406677, -0.346179336309433, 0.2015683650970459], [-0.4188190698623657, -0.5016990900039673, 0.2140047252178192, -0.06029246747493744], [-0.021660931408405304, 0.04751019552350044, -0.6953786015510559, -0.5345582962036133], [0.5547587275505066, 0.2320217788219452, 0.44901153445243835, -0.8750210404396057], [0.9238684177398682, 0.5569440722465515, -0.41409748792648315, -0.8329393863677979], [0.34989067912101746, -0.27599990367889404, -0.28253260254859924, -0.5326759815216064], [-0.3412042260169983, 0.2241271734237671, 0.5662476420402527, 0.6489465236663818], [0.6678473949432373, 0.8216654658317566, 0.2860565483570099, -0.24066384136676788], [-0.5879958868026733, -0.3150010108947754, -0.1729123890399933, -0.0035123659763485193], [-0.24676518142223358, 0.47698840498924255, -0.3594236373901367, -0.274384081363678], [0.32170069217681885, -0.5841577053070068, -0.050966549664735794, -0.036482132971286774]]\n",
      "\n",
      "0.bias:\n",
      "[-0.2886911928653717, -0.9611756205558777, 0.12823690474033356, -0.29152652621269226, 0.42256811261177063, -0.17650112509727478, 0.5008096694946289, 0.2104753702878952, 0.19745533168315887, 0.003778302576392889, 0.41693538427352905, 0.2357356995344162, -0.2029118686914444, 0.009237002581357956, -0.44034039974212646, -0.3672792911529541]\n",
      "\n",
      "2.weight:\n",
      "[[-0.06512260437011719, -0.233586385846138, 0.7831387519836426, -0.12105271965265274, 0.31757593154907227, -0.4016984701156616, 0.2631053626537323, -0.2578529715538025, -0.5768530964851379, -0.2874746024608612, -0.19478186964988708, 0.40693962574005127, -0.6581087708473206, 0.4327957034111023, -0.13715921342372894, -0.22016283869743347], [-0.03405812755227089, -0.05181296169757843, 0.2153063416481018, 0.08257037401199341, 0.004884743597358465, 0.12110190093517303, -0.05032290145754814, 0.4467010796070099, 0.4690004885196686, -0.621935248374939, 0.135374516248703, -0.05250447988510132, 0.24567733705043793, -0.032604388892650604, -0.09549624472856522, 0.11971085518598557], [-0.006516152061522007, -0.14536704123020172, -0.05622216686606407, 0.06997157633304596, 0.2700374126434326, -0.07104870676994324, 0.235897034406662, -0.774582028388977, 0.3553144931793213, 0.6713795065879822, -0.6621872186660767, -0.2764653265476227, 0.26632243394851685, -0.22919288277626038, -0.7858787178993225, 0.10129084438085556]]\n",
      "\n",
      "2.bias:\n",
      "[-0.055789485573768616, -0.14767025411128998, -0.5413630604743958]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "state_dict_serializable = OrderedDict((key, value.numpy().tolist()) for key, value in state_dict.items())\n",
    "\n",
    "# The `weights_and_biases_np` now contains the weights and biases as NumPy arrays\n",
    "for key, value in state_dict_serializable.items():\n",
    "    print(f\"{key}:\\n{value}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict cached to server, loading to kernel...\n",
      "Dict loaded successfully to the kernel\n",
      "\n"
     ]
    }
   ],
   "source": [
    "session.private_import(state_dict_serializable, 'state_dict_serializable')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "The `private_import` function is used to load the serialised state dictionary back into the environment, ensuring it is ready for use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recreating the Model\n",
    "We'll recreate the model architecture and load the saved parameters to restore the trained model. üõ†Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ag\n",
    "model = nn.Sequential(nn.Linear(4, 16),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(16, 3),\n",
    "                     nn.Softmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ag\n",
    "import numpy as np\n",
    "for k, v in state_dict_serializable.items():\n",
    "  state_dict_serializable[k] = torch.from_numpy(np.array(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "- We recreate the neural network architecture to match the original model structure. This is necessary for loading the state dictionary correctly.\n",
    "- We convert the serialised state dictionary values from lists back to torch tensors to ensure compatibility with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ag\n",
    "model.load_state_dict(state_dict_serializable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "The load_state_dict method loads the saved parameters into the recreated model, restoring it to its trained state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping Up\n",
    "Finally, let's end the session. It's always good to clean up when you're done. üßπ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "session.terminate_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "And that's a wrap! üéâ\n",
    "\n",
    "- This notebook provided a comprehensive guide to implementing differential privacy in machine learning workflows using Antigranular.\n",
    "- We demonstrated that it is possible to train accurate models while preserving data privacy, which is crucial for applications involving sensitive information.\n",
    "- The ability to save and reload the model ensures that the trained model can be reused without retraining, enhancing efficiency and practicality.\n",
    "\n",
    "By following the steps outlined in this notebook, you can build, train, and evaluate your own differentially private machine learning models, ensuring both accuracy and privacy in your data-driven applications. Woohoo! üíª"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
