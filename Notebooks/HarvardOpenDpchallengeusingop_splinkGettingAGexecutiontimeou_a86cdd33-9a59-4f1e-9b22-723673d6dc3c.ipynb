{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_mWBHwQymLQ"
      },
      "source": [
        "# Sample Example: SPLink"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnZB8JliVADH"
      },
      "source": [
        "We provide two libraries for record linking, RecordLinkage and SPLink. Follow the RecordLinkage notebook [here](https://www.antigranular.com/notebooks/651a938d0f1e51b4fa0b651a)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CgLEIhZy5s7"
      },
      "source": [
        "SPLink is a Python package designed for probabilistic record linkage, and like Record Linkage, plays a crucial part in linking records and deduplicating datasets.\n",
        "\n",
        "Participants can use SPLink to predict which rows link together and then further cluster these connections to generate an Individual ID. This can prove especially useful when unique identifiers are missing or differ significantly across datasets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rmlact2Yyziv"
      },
      "source": [
        "## Getting Started: Setting Up the Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPnrlAizqLgV",
        "outputId": "138b1bd7-ddc2-475f-ec3c-2c863a725f83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: antigranular in /usr/local/lib/python3.10/dist-packages (0.2.16)\n",
            "Requirement already satisfied: diffprivlib<0.7.0,>=0.6.2 in /usr/local/lib/python3.10/dist-packages (from antigranular) (0.6.3)\n",
            "Requirement already satisfied: ipython<8.0.0,>=7.34.0 in /usr/local/lib/python3.10/dist-packages (from antigranular) (7.34.0)\n",
            "Requirement already satisfied: oblv-client<0.2.0,>=0.1.15 in /usr/local/lib/python3.10/dist-packages (from antigranular) (0.1.15)\n",
            "Requirement already satisfied: onnx<2.0.0,>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from antigranular) (1.15.0)\n",
            "Requirement already satisfied: pandas<2.0.0,>=1.5.3 in /usr/local/lib/python3.10/dist-packages (from antigranular) (1.5.3)\n",
            "Requirement already satisfied: pydantic<2.0.0,>=1.10.7 in /usr/local/lib/python3.10/dist-packages (from antigranular) (1.10.13)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from antigranular) (2.31.0)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from diffprivlib<0.7.0,>=0.6.2->antigranular) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn>=0.24.2 in /usr/local/lib/python3.10/dist-packages (from diffprivlib<0.7.0,>=0.6.2->antigranular) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.10/dist-packages (from diffprivlib<0.7.0,>=0.6.2->antigranular) (1.11.4)\n",
            "Requirement already satisfied: joblib>=0.16.0 in /usr/local/lib/python3.10/dist-packages (from diffprivlib<0.7.0,>=0.6.2->antigranular) (1.3.2)\n",
            "Requirement already satisfied: setuptools>=49.0.0 in /usr/local/lib/python3.10/dist-packages (from diffprivlib<0.7.0,>=0.6.2->antigranular) (67.7.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython<8.0.0,>=7.34.0->antigranular) (0.19.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython<8.0.0,>=7.34.0->antigranular) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython<8.0.0,>=7.34.0->antigranular) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython<8.0.0,>=7.34.0->antigranular) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython<8.0.0,>=7.34.0->antigranular) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython<8.0.0,>=7.34.0->antigranular) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython<8.0.0,>=7.34.0->antigranular) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython<8.0.0,>=7.34.0->antigranular) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython<8.0.0,>=7.34.0->antigranular) (4.9.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from oblv-client<0.2.0,>=0.1.15->antigranular) (2.0.7)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx<2.0.0,>=1.14.0->antigranular) (3.20.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.0.0,>=1.5.3->antigranular) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.0.0,>=1.5.3->antigranular) (2023.3.post1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.0.0,>=1.10.7->antigranular) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->antigranular) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->antigranular) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->antigranular) (2023.11.17)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython<8.0.0,>=7.34.0->antigranular) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython<8.0.0,>=7.34.0->antigranular) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<8.0.0,>=7.34.0->antigranular) (0.2.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas<2.0.0,>=1.5.3->antigranular) (1.16.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.2->diffprivlib<0.7.0,>=0.6.2->antigranular) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install antigranular"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "id": "dw1cOjTozPAf",
        "outputId": "0ce8d359-c735-46db-cd51-ff5e3abdf9c4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset \"Flight Company Dataset\" loaded to the kernel as \u001b[92mflight_company_dataset\u001b[0m\n",
            "\n",
            "Dataset \"Health Organisation Dataset\" loaded to the kernel as \u001b[92mhealth_organisation_dataset\u001b[0m\n",
            "\n",
            "Connected to Antigranular server session id: a86cdd33-9a59-4f1e-9b22-723673d6dc3c, the session will time out if idle for 25 minutes\n",
            "Cell magic '%%ag' registered successfully, use `%%ag` in a notebook cell to execute your python code on Antigranular private python server\n",
            "ðŸš€ Everything's set up and ready to roll!\n"
          ]
        }
      ],
      "source": [
        "import antigranular as ag\n",
        "session = ag.login(<client_id>, <client_secret>, competition = \"Harvard OpenDP Hackathon\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxG_yEJ_y3RO"
      },
      "source": [
        "### Importing the Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQaU004bzjHB"
      },
      "source": [
        "***In this competition we are provided with two datasets:***\n",
        "\n",
        "The airline companies have information about passengers and their travel dates (`flight_company_dataset_for_sandbox`), and the national health organisation has records of patients who did the COVID test and whether their result was positive or negative (`health_organisation_dataset_for_sandbox`).\n",
        "\n",
        "These are already provided within the AG environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "bnKr4clsqNUY"
      },
      "outputs": [],
      "source": [
        "%%ag\n",
        "health = health_organisation_dataset\n",
        "flight = flight_company_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **parse_date()** function standardizes diverse date formats within Flight and Health datasets. It transforms various date representations, such as 'DD-MM-YY', 'MM/YYYY/DD', 'YYYY.MM.DD', etc., into a consistent 'YYYY-MM-DD' format.\n",
        "\n",
        "This process employs regular expressions and a month dictionary to accurately identify and convert dates. The function then applies these standardized dates to specific columns in the datasets to ensure consistency and streamline analysis."
      ],
      "metadata": {
        "id": "IdDpnykuSmrg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4lJDXVopsHB",
        "outputId": "2b40eb83-7447-40ce-cd67-b5d2edbbd5d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1978-10-02\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%ag\n",
        "import re\n",
        "\n",
        "def parse_date(date_str: str) ->str:\n",
        "    months = {\n",
        "        'Jan': '01', 'Feb': '02', 'Mar': '03', 'Apr': '04', 'May': '05', 'Jun': '06',\n",
        "        'Jul': '07', 'Aug': '08', 'Sep': '09', 'Oct': '10', 'Nov': '11', 'Dec': '12',\n",
        "        'January': '01', 'February': '02', 'March': '03', 'April': '04', 'May': '05',\n",
        "        'June': '06', 'July': '07', 'August': '08', 'September': '09', 'October': '10',\n",
        "        'November': '11', 'December': '12',\n",
        "        'Sept': '09', 'Sep': '09',  # Added 'Sept' and 'Sep'\n",
        "    }\n",
        "\n",
        "    separators = [' ', '/', '.', '-']\n",
        "    thirty_day_months = {4, 6, 9, 11}\n",
        "    # Check for formats like 'Oct-12-02 , Oct.12.02 , Oct 12 02 , Oct/12/02'\n",
        "    for separator in separators:\n",
        "        parts = re.split(f'[{separator}]', date_str)\n",
        "        if len(parts) == 3:\n",
        "            month = months.get(parts[0])\n",
        "            if month:\n",
        "                year = parts[2]\n",
        "                if len(year) == 2:\n",
        "                    if int(year) < 21:\n",
        "                        year = '20' + year\n",
        "                    else:\n",
        "                        year = '19' + year\n",
        "                day = int(parts[1])\n",
        "                if (\n",
        "                    1 <= day <= 31 and\n",
        "                    (int(month) != 2 or 1 <= day <= 28) and  # February\n",
        "                    (int(month) not in thirty_day_months or 1 <= day <= 30)  # Months with 30 days\n",
        "                ):\n",
        "                  return f\"{year}-{month}-{parts[1].zfill(2)}\"\n",
        "\n",
        "    # Check for formats like 'Oct-1978-12 , Oct.1978.12 , Oct 1978 12 , Oct/1978/12'\n",
        "    for separator in separators:\n",
        "        parts = re.split(f'[{separator}]', date_str)\n",
        "        if len(parts) == 3:\n",
        "            month = months.get(parts[0])\n",
        "            if month:\n",
        "                year = parts[1]\n",
        "                if len(year) == 2:\n",
        "                    if int(year) < 21:\n",
        "                        year = '20' + year\n",
        "                    else:\n",
        "                        year = '19' + year\n",
        "                day = int(parts[2])\n",
        "                if (\n",
        "                    1 <= day <= 31 and\n",
        "                    (int(month) != 2 or 1 <= day <= 28) and  # February\n",
        "                    (int(month) not in thirty_day_months or 1 <= day <= 30)  # Months with 30 days\n",
        "                ):\n",
        "                  return f\"{year}-{month}-{parts[2].zfill(2)}\"\n",
        "\n",
        "    # Check for formats like '01.Sep.1990 , 01/Sep/1990 , 01 Sep 1990 , 01-Sep-1990'\n",
        "    for separator in separators:\n",
        "        parts = re.split(f'[{separator}]', date_str)\n",
        "        if len(parts) == 3:\n",
        "            month = months.get(parts[1])\n",
        "            if month:\n",
        "                year = parts[2]\n",
        "                if len(year) == 2:\n",
        "                    if int(year) < 21:\n",
        "                        year = '20' + year\n",
        "                    else:\n",
        "                        year = '19' + year\n",
        "                day = int(parts[0])\n",
        "                if (\n",
        "                    1 <= day <= 31 and\n",
        "                    (int(month) != 2 or 1 <= day <= 28) and  # February\n",
        "                    (int(month) not in thirty_day_months or 1 <= day <= 30)  # Months with 30 days\n",
        "                ):\n",
        "                  return f\"{year}-{month}-{parts[0].zfill(2)}\"\n",
        "\n",
        "    # Check for formats like '1990.Sep.01 , 1990/Sep/01 , 1990 Sep 01 , 1990-Sep-01'\n",
        "    for separator in separators:\n",
        "        parts = re.split(f'[{separator}]', date_str)\n",
        "        if len(parts) == 3:\n",
        "            month = months.get(parts[1])\n",
        "            if month:\n",
        "                year = parts[0]\n",
        "                if len(year) == 2:\n",
        "                    if int(year) < 21:\n",
        "                        year = '20' + year\n",
        "                    else:\n",
        "                        year = '19' + year\n",
        "                day = int(parts[2])\n",
        "                if (\n",
        "                    1 <= day <= 31 and\n",
        "                    (int(month) != 2 or 1 <= day <= 28) and  # February\n",
        "                    (int(month) not in thirty_day_months or 1 <= day <= 30)  # Months with 30 days\n",
        "                ):\n",
        "                  return f\"{year}-{month}-{parts[2].zfill(2)}\"\n",
        "\n",
        "    # Check for formats like '1990.01.Sept , 1990/01/Sept , 1990 01 Sept , 1990-01-Sept'\n",
        "    for separator in separators:\n",
        "        parts = re.split(f'[{separator}]', date_str)\n",
        "        if len(parts) == 3:\n",
        "            month = months.get(parts[2])\n",
        "            if month:\n",
        "                year = parts[0]\n",
        "                if len(year) == 2:\n",
        "                    if int(year) < 21:\n",
        "                        year = '20' + year\n",
        "                    else:\n",
        "                        year = '19' + year\n",
        "                day = int(parts[1])\n",
        "                if (\n",
        "                    1 <= day <= 31 and\n",
        "                    (int(month) != 2 or 1 <= day <= 28) and  # February\n",
        "                    (int(month) not in thirty_day_months or 1 <= day <= 30)  # Months with 30 days\n",
        "                ):\n",
        "                  return f\"{year}-{month}-{parts[1].zfill(2)}\"\n",
        "    # Check for formats like '01.1990.Sept , 01/1990/Sept , 01 1990 Sept , 01-1990-Sept'\n",
        "    for separator in separators:\n",
        "        parts = re.split(f'[{separator}]', date_str)\n",
        "        if len(parts) == 3:\n",
        "            month = months.get(parts[2])\n",
        "            if month:\n",
        "                year = parts[1]\n",
        "                if len(year) == 2:\n",
        "                    if int(year) < 21:\n",
        "                        year = '20' + year\n",
        "                    else:\n",
        "                        year = '19' + year\n",
        "                day = int(parts[0])\n",
        "                if (\n",
        "                    1 <= day <= 31 and\n",
        "                    (int(month) != 2 or 1 <= day <= 28) and  # February\n",
        "                    (int(month) not in thirty_day_months or 1 <= day <= 30)  # Months with 30 days\n",
        "                ):\n",
        "                  return f\"{year}-{month}-{parts[0].zfill(2)}\"\n",
        "\n",
        "    # Check for formats like '10-12-02 , 10.12.02 , 10 12 02 , 10/12/02'\n",
        "    for separator in separators:\n",
        "        parts = re.split(f'[{separator}]', date_str)\n",
        "        if len(parts) == 3:\n",
        "            month = parts[0]\n",
        "            if 1 <= int(month) <= 12:\n",
        "                year = parts[2]\n",
        "                if len(year) == 2:\n",
        "                    if int(year) < 21:\n",
        "                        year = '20' + year\n",
        "                    else:\n",
        "                        year = '19' + year\n",
        "                day = int(parts[1])\n",
        "                if (\n",
        "                    1 <= day <= 31 and\n",
        "                    (int(month) != 2 or 1 <= day <= 28) and  # February\n",
        "                    (int(month) not in thirty_day_months or 1 <= day <= 30)  # Months with 30 days\n",
        "                ):\n",
        "                  return f\"{year}-{month}-{parts[1].zfill(2)}\"\n",
        "\n",
        "    # Check for formats like '10-1978-12 , 10.1978.12 , 10 1978 12 , 10/1978/12'\n",
        "    for separator in separators:\n",
        "        parts = re.split(f'[{separator}]', date_str)\n",
        "        if len(parts) == 3:\n",
        "            month = parts[0]\n",
        "            if 1 <= int(month) <= 12:\n",
        "                year = parts[1]\n",
        "                if len(year) == 2:\n",
        "                    if int(year) < 21:\n",
        "                        year = '20' + year\n",
        "                    else:\n",
        "                        year = '19' + year\n",
        "                day = int(parts[2])\n",
        "                if (\n",
        "                    1 <= day <= 31 and\n",
        "                    (int(month) != 2 or 1 <= day <= 28) and  # February\n",
        "                    (int(month) not in thirty_day_months or 1 <= day <= 30)  # Months with 30 days\n",
        "                ):\n",
        "                  return f\"{year}-{month}-{parts[2].zfill(2)}\"\n",
        "\n",
        "    # Check for formats like '01.09.1990 , 01/09/1990 , 01 09 1990 , 01-09-1990'\n",
        "    for separator in separators:\n",
        "        parts = re.split(f'[{separator}]', date_str)\n",
        "        if len(parts) == 3:\n",
        "            month = parts[1]\n",
        "            if 1 <= int(month) <= 12:\n",
        "                year = parts[2]\n",
        "                if len(year) == 2:\n",
        "                    if int(year) < 21:\n",
        "                        year = '20' + year\n",
        "                    else:\n",
        "                        year = '19' + year\n",
        "                day = int(parts[0])\n",
        "                if (\n",
        "                    1 <= day <= 31 and\n",
        "                    (int(month) != 2 or 1 <= day <= 28) and  # February\n",
        "                    (int(month) not in thirty_day_months or 1 <= day <= 30)  # Months with 30 days\n",
        "                ):\n",
        "                  return f\"{year}-{month}-{parts[0].zfill(2)}\"\n",
        "\n",
        "    # Check for formats like '1990.09.01 , 1990/09/01 , 1990 09 01 , 1990-09-01'\n",
        "    for separator in separators:\n",
        "        parts = re.split(f'[{separator}]', date_str)\n",
        "        if len(parts) == 3:\n",
        "            month = parts[1]\n",
        "            if 1 <= int(month) <= 12:\n",
        "                year = parts[0]\n",
        "                if len(year) == 2:\n",
        "                    if int(year) < 21:\n",
        "                        year = '20' + year\n",
        "                    else:\n",
        "                        year = '19' + year\n",
        "                day = int(parts[2])\n",
        "                if (\n",
        "                    1 <= day <= 31 and\n",
        "                    (int(month) != 2 or 1 <= day <= 28) and  # February\n",
        "                    (int(month) not in thirty_day_months or 1 <= day <= 30)  # Months with 30 days\n",
        "                ):\n",
        "                  return f\"{year}-{month}-{parts[2].zfill(2)}\"\n",
        "\n",
        "    # Check for formats like '1990.01.09 , 1990/01/09 , 1990 01 09 , 1990-01-09'\n",
        "    for separator in separators:\n",
        "        parts = re.split(f'[{separator}]', date_str)\n",
        "        if len(parts) == 3:\n",
        "            month = parts[2]\n",
        "            if 1 <= int(month) <= 12:\n",
        "                year = parts[0]\n",
        "                if len(year) == 2:\n",
        "                    if int(year) < 21:\n",
        "                        year = '20' + year\n",
        "                    else:\n",
        "                        year = '19' + year\n",
        "                day = int(parts[1])\n",
        "                if (\n",
        "                    1 <= day <= 31 and\n",
        "                    (int(month) != 2 or 1 <= day <= 28) and  # February\n",
        "                    (int(month) not in thirty_day_months or 1 <= day <= 30)  # Months with 30 days\n",
        "                ):\n",
        "                  return f\"{year}-{month}-{parts[1].zfill(2)}\"\n",
        "    # Check for formats like '01.1990.09 , 01/1990/09 , 01 1990 09 , 01-1990-09'\n",
        "    for separator in separators:\n",
        "        parts = re.split(f'[{separator}]', date_str)\n",
        "        if len(parts) == 3:\n",
        "            month = parts[2]\n",
        "            if 1 <= int(month) <= 12:\n",
        "                year = parts[1]\n",
        "                if len(year) == 2:\n",
        "                    if int(year) < 21:\n",
        "                        year = '20' + year\n",
        "                    else:\n",
        "                        year = '19' + year\n",
        "                day = int(parts[0])\n",
        "                if (\n",
        "                    1 <= day <= 31 and\n",
        "                    (int(month) != 2 or 1 <= day <= 28) and  # February\n",
        "                    (int(month) not in thirty_day_months or 1 <= day <= 30)  # Months with 30 days\n",
        "                ):\n",
        "                  return f\"{year}-{month}-{parts[0].zfill(2)}\"\n",
        "\n",
        "    return f\"Invalid Input Error: Could not parse string '{date_str}' according to format specifier '%Y-%m-%d'\"\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "date_str = \"10-78-02\"\n",
        "parsed_date = parse_date(date_str)\n",
        "ag_print(parsed_date)\n",
        "\n",
        "# Standardize date format in flight dataset\n",
        "flight['passenger_date_of_birth'] = flight['passenger_date_of_birth'].map(parse_date)\n",
        "# Standardize date format in health dataset\n",
        "health['patient_date_of_birth'] = health['patient_date_of_birth'].map(parse_date)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **normalize_phone_number** function tidies up phone numbers by removing non-numeric characters and leading zeros. It's applied to the 'passenger_phone_number' in the Flight dataset and 'patient_phone_number' in the Health dataset to standardize phone number formats for consistency in analysis."
      ],
      "metadata": {
        "id": "revCqkZiSpVh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "tDb3hYFsCJFo"
      },
      "outputs": [],
      "source": [
        "%%ag\n",
        "import re\n",
        "\n",
        "def normalize_phone_number(phone:str)->str:\n",
        "    # Remove non-numeric characters and spaces\n",
        "    phone = re.sub(r'\\D', '', phone)\n",
        "    return phone\n",
        "\n",
        "flight['passenger_phone_number'] = flight['passenger_phone_number'].map(normalize_phone_number)\n",
        "# Standardize date format in health dataset\n",
        "health['patient_phone_number'] = health['patient_phone_number'].map(normalize_phone_number)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bn7zAPjC1apL"
      },
      "source": [
        "## Pre Processing the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "GBBnrWrrquvt"
      },
      "outputs": [],
      "source": [
        "%%ag\n",
        "import numpy as np\n",
        "import op_pandas as opd\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IexNGwnQ1lpG"
      },
      "source": [
        "### **Splink requires that you clean your data and assign unique IDs to rows before linking**\n",
        "\n",
        "* **Unique IDs:** Each input dataset must have a unique ID column, which is unique within the dataset. By default, Splink assumes this column will be called unique_id.\n",
        "* **Conformant input datasets:** Input datasets must be conformant, meaning they share the same column names and data formats.\n",
        "* **Cleaning:** Ensure data consistency by cleaning your data. This process includes standardising date formats, matching text case, and handling invalid data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynvahsPb2qpC"
      },
      "source": [
        "### Creating Unique IDs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQodZo-y2tHn"
      },
      "source": [
        "Since the number of records in both the datasets is public information, we can use these to create the `unique_id` columns.\n",
        "\n",
        "For this, we use `numpy.arange`, which generates evenly spaced values within a given interval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "E0mosknM1jxC"
      },
      "outputs": [],
      "source": [
        "%%ag\n",
        "num_health = 71707 #taken from competition page\n",
        "num_flight = 85242 # taken from competition page\n",
        "\n",
        "unique_id_health = opd.PrivateSeries(pd.Series(np.arange(num_health)))\n",
        "unique_id_flight = opd.PrivateSeries(pd.Series(np.arange(num_flight)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "cjgtY6r1quyh"
      },
      "outputs": [],
      "source": [
        "%%ag\n",
        "health['unique_id'] = unique_id_health\n",
        "flight['unique_id'] = unique_id_flight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wglSKxXh38jU"
      },
      "source": [
        "### Conforming Input Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQ-47A-A3Ozd"
      },
      "source": [
        "In order to conform the various column names, let us examine what they are."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l09nS8BPqu1d",
        "outputId": "a976eff3-044f-4cbe-98d8-cef1354cc4b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Health Columns:\n",
            "['patient_firstname', 'patient_lastname', 'patient_date_of_birth', 'patient_phone_number', 'patient_email_address', 'covidtest_date', 'covidtest_result', 'patient_address', 'unique_id']\n",
            "Flight Columns:\n",
            "['flight_number', 'flight_date', 'flight_from', 'flight_to', 'passenger_firstname', 'passenger_lastname', 'passenger_date_of_birth', 'passenger_phone_number', 'passenger_email_address', 'unique_id']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%ag\n",
        "ag_print(\"Health Columns:\")\n",
        "ag_print(health.columns)\n",
        "ag_print(\"Flight Columns:\")\n",
        "ag_print(flight.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWXDqbh-4Kkg"
      },
      "source": [
        "As we can see, the column names are not the same in both of the datasets. For example, first name in the health dataset is `patient_firstname` and in the flight dataset it is `passenger_firstname`.\n",
        "\n",
        "We also want to link on basis of dates (`covidtest_date` in Health Dataset and `flight_date` in Flight Dataset).\n",
        "\n",
        "Hence, we will write a function which makes the same columns of the same name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "KBURNDVUqu4V"
      },
      "outputs": [],
      "source": [
        "%%ag\n",
        "def conform_columns(df: opd.PrivateDataFrame) -> opd.PrivateDataFrame:\n",
        "    final_columns = []\n",
        "    for col in df.columns:\n",
        "        if \"firstname\" in col:  # converting patient_firstname and passenger_firstname -> firstname\n",
        "            final_columns.append(\"firstname\")\n",
        "            df[\"firstname\"] = df[col]\n",
        "        elif \"lastname\" in col:  # converting patient_lastname and passenger_lastname -> lastname\n",
        "            final_columns.append(\"lastname\")\n",
        "            df[\"lastname\"] = df[col]\n",
        "        elif \"date_of_birth\" in col: # converting patient_date_of_birth and ppassenger_date_of_birth -> date_of_birth\n",
        "            final_columns.append(\"date_of_birth\")\n",
        "            df[\"date_of_birth\"] = df[col]\n",
        "        elif \"covidtest_date\" in col: # converting covidtest_date and flight_date -> date\n",
        "            final_columns.append(\"date\")\n",
        "            df[\"date\"] = df[col]\n",
        "        elif \"flight_date\" in col:\n",
        "            final_columns.append(\"date\")\n",
        "            df[\"date\"] = df[col]\n",
        "        elif \"phone_number\" in col:\n",
        "            final_columns.append(\"phone_number\")\n",
        "            df[\"phone_number\"] = df[col]\n",
        "        elif \"email_address\" in col:\n",
        "            final_columns.append(\"email_address\")\n",
        "            df[\"email_address\"] = df[col]\n",
        "        else:\n",
        "            final_columns.append(col)\n",
        "\n",
        "    df = df[final_columns]\n",
        "    return df\n",
        "\n",
        "health = conform_columns(health)\n",
        "flight = conform_columns(flight)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RDk-5mpKjtg"
      },
      "source": [
        "We only need the records where covidtest_result is positive, so we will extract them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wOp6sR2qu7E",
        "outputId": "6a2e66e3-b3e6-4aa3-ccfe-625595f9ac50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/code/dependencies/op_pandas/op_pandas/core/private_dataframe.py:512: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self.df[key] = value._series\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%ag\n",
        "# Lets remove those passenger records who tested negative.\n",
        "health['covidtest_result'] = health['covidtest_result'].where(health['covidtest_result'] == 'positive')\n",
        "health = health.dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gEPFgkFK-lr"
      },
      "source": [
        "Checking out the columns again, we can see that they are conformant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpV31l3Cqu91",
        "outputId": "3eec68aa-e5d0-4733-8dd2-6f5573f4cbbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Health Columns:\n",
            "['firstname', 'lastname', 'date_of_birth', 'phone_number', 'email_address', 'date', 'covidtest_result', 'patient_address', 'unique_id']\n",
            "Flight Columns:\n",
            "['flight_number', 'date', 'flight_from', 'flight_to', 'firstname', 'lastname', 'date_of_birth', 'phone_number', 'email_address', 'unique_id']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%ag\n",
        "ag_print(\"Health Columns:\")\n",
        "ag_print(health.columns)\n",
        "ag_print(\"Flight Columns:\")\n",
        "ag_print(flight.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_p0YOzTgKtIc"
      },
      "source": [
        "Now we can see that the column name is conformant. But there are some columns which we don't need for our analysis, so let us remove those."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "aqitCMGTqvAa"
      },
      "outputs": [],
      "source": [
        "%%ag\n",
        "health_link = health[['firstname', 'lastname', 'date_of_birth', 'date','phone_number','email_address','unique_id']]\n",
        "flight_link = flight[['firstname', 'lastname', 'date_of_birth', 'date','phone_number','email_address','unique_id']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xBIV0pFLlu1"
      },
      "source": [
        "## Comparisons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZJM6lMENKSA"
      },
      "source": [
        "A key feature of Splink is the ability to customise how record comparisons are made - that is, how similarity is defined for different data types.\n",
        "\n",
        "By tailoring the definitions of similarity, linking models are more effectively able to distinguish beteween different gradations of similarity, leading to more accurate data linking models.\n",
        "\n",
        "For more information on comparisons, follow [this link](https://moj-analytical-services.github.io/splink/topic_guides/comparisons/customising_comparisons.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-JRPJZgNpwW"
      },
      "source": [
        "Here, we will create 4 comparisons:\n",
        "\n",
        "* Fuzzy matching of `firstname`\n",
        "* Fuzzy matching of `lastname`\n",
        "* Fuzzy matching of `phonenumbers`\n",
        "* Difference of `date` column to be within 14 days\n",
        "* Fuzzy matching of `date_of_birth` column\n",
        "*Fuzzy matching of `email_address`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "AHmKAGN7rVL6"
      },
      "outputs": [],
      "source": [
        "%%ag\n",
        "import op_splink.duckdb.comparison_template_library as ctl\n",
        "from op_splink.duckdb.blocking_rule_library import block_on\n",
        "\n",
        "bespoke_email_comparison = ctl.email_comparison(\n",
        "    \"email_address\",\n",
        "    jaro_winkler_thresholds=[],\n",
        "    damerau_levenshtein_thresholds=[3],\n",
        "    include_username_match_level=False,\n",
        "    include_domain_match_level=True,\n",
        "    invalid_emails_as_null=True,\n",
        ")\n",
        "\n",
        "first_name_comparison = ctl.name_comparison(\"firstname\")\n",
        "last_name_comparison = ctl.name_comparison(\"lastname\", jaro_winkler_thresholds=[0.8])\n",
        "phone_number_comparison=ctl.name_comparison(\"phone_number\",damerau_levenshtein_thresholds = [])\n",
        "date_difference = ctl.date_comparison(\"date\", cast_strings_to_date = True, include_exact_match_level = False, damerau_levenshtein_thresholds = [], datediff_thresholds = [14], datediff_metrics = [\"day\"])\n",
        "date_of_birth_comparison = ctl.date_comparison(\"date_of_birth\", cast_strings_to_date = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5edrqDM5OCYW"
      },
      "source": [
        "SPLink uses SQL to find the comparisons. We can use `human_readable_description` method to check the comparison levels and the sql rules for the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "achUtdjorVPV",
        "outputId": "f440e099-fc08-470f-d054-5ce85aa066bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparison 'Exact match vs. Firstname within levenshtein threshold 1 vs. Firstname within damerau-levenshtein threshold 1 vs. Firstname within jaro_winkler thresholds 0.9, 0.8 vs. anything else' of \"firstname\".\n",
            "Similarity is assessed using the following ComparisonLevels:\n",
            "    - 'Null' with SQL rule: \"firstname_l\" IS NULL OR \"firstname_r\" IS NULL\n",
            "    - 'Exact match firstname' with SQL rule: \"firstname_l\" = \"firstname_r\"\n",
            "    - 'Damerau_levenshtein <= 1' with SQL rule: damerau_levenshtein(\"firstname_l\", \"firstname_r\") <= 1\n",
            "    - 'Jaro_winkler_similarity >= 0.9' with SQL rule: jaro_winkler_similarity(\"firstname_l\", \"firstname_r\") >= 0.9\n",
            "    - 'Jaro_winkler_similarity >= 0.8' with SQL rule: jaro_winkler_similarity(\"firstname_l\", \"firstname_r\") >= 0.8\n",
            "    - 'All other comparisons' with SQL rule: ELSE\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%ag\n",
        "ag_print(first_name_comparison.human_readable_description)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pEbcJnCQHYl",
        "outputId": "d9b8446e-92e9-44eb-b709-9f1e474d9eba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparison 'Exact match vs. Lastname within levenshtein threshold 1 vs. Lastname within damerau-levenshtein threshold 1 vs. Lastname within jaro_winkler threshold 0.8 vs. anything else' of \"lastname\".\n",
            "Similarity is assessed using the following ComparisonLevels:\n",
            "    - 'Null' with SQL rule: \"lastname_l\" IS NULL OR \"lastname_r\" IS NULL\n",
            "    - 'Exact match lastname' with SQL rule: \"lastname_l\" = \"lastname_r\"\n",
            "    - 'Damerau_levenshtein <= 1' with SQL rule: damerau_levenshtein(\"lastname_l\", \"lastname_r\") <= 1\n",
            "    - 'Jaro_winkler_similarity >= 0.8' with SQL rule: jaro_winkler_similarity(\"lastname_l\", \"lastname_r\") >= 0.8\n",
            "    - 'All other comparisons' with SQL rule: ELSE\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%ag\n",
        "ag_print(last_name_comparison.human_readable_description)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%ag\n",
        "ag_print(phone_number_comparison.human_readable_description)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0N0_TAtUsc1",
        "outputId": "3ffed0d0-716a-4413-804a-1e4f6867a762"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparison 'Exact match vs. Phone_Number within jaro_winkler thresholds 0.9, 0.8 vs. anything else' of \"phone_number\".\n",
            "Similarity is assessed using the following ComparisonLevels:\n",
            "    - 'Null' with SQL rule: \"phone_number_l\" IS NULL OR \"phone_number_r\" IS NULL\n",
            "    - 'Exact match phone_number' with SQL rule: \"phone_number_l\" = \"phone_number_r\"\n",
            "    - 'Jaro_winkler_similarity >= 0.9' with SQL rule: jaro_winkler_similarity(\"phone_number_l\", \"phone_number_r\") >= 0.9\n",
            "    - 'Jaro_winkler_similarity >= 0.8' with SQL rule: jaro_winkler_similarity(\"phone_number_l\", \"phone_number_r\") >= 0.8\n",
            "    - 'All other comparisons' with SQL rule: ELSE\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%ag\n",
        "ag_print(date_difference.human_readable_description)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIaJ-_4AUogg",
        "outputId": "5862db29-cb5f-4d8e-bf5e-8171bf890d07"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparison 'Dates within the following threshold Day(s): 14 vs. anything else' of \"date\".\n",
            "Similarity is assessed using the following ComparisonLevels:\n",
            "    - 'Null' with SQL rule: \"date_l\" IS NULL OR \"date_r\" IS NULL\n",
            "    - 'Within 14 days' with SQL rule: \n",
            "            abs(date_diff('day',\n",
            "                strptime(\"date_l\", '%Y-%m-%d'),\n",
            "                strptime(\"date_r\", '%Y-%m-%d'))\n",
            "                ) <= 14\n",
            "        \n",
            "    - 'All other comparisons' with SQL rule: ELSE\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%ag\n",
        "ag_print(date_of_birth_comparison.human_readable_description)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oyJNilpU6Se",
        "outputId": "c252a4e0-67e2-4205-86b8-f1c2803b587a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparison 'Exact match vs. Date_Of_Birth within damerau-levenshtein threshold 1 vs. Dates within the following thresholds Month(s): 1, Year(s): 1, Year(s): 10 vs. anything else' of \"date_of_birth\".\n",
            "Similarity is assessed using the following ComparisonLevels:\n",
            "    - 'Null' with SQL rule: \"date_of_birth_l\" IS NULL OR \"date_of_birth_r\" IS NULL\n",
            "    - 'Exact match' with SQL rule: \"date_of_birth_l\" = \"date_of_birth_r\"\n",
            "    - 'Damerau_levenshtein <= 1' with SQL rule: damerau_levenshtein(\"date_of_birth_l\", \"date_of_birth_r\") <= 1\n",
            "    - 'Within 1 month' with SQL rule: \n",
            "            abs(date_diff('month',\n",
            "                strptime(\"date_of_birth_l\", '%Y-%m-%d'),\n",
            "                strptime(\"date_of_birth_r\", '%Y-%m-%d'))\n",
            "                ) <= 1\n",
            "        \n",
            "    - 'Within 1 year' with SQL rule: \n",
            "            abs(date_diff('year',\n",
            "                strptime(\"date_of_birth_l\", '%Y-%m-%d'),\n",
            "                strptime(\"date_of_birth_r\", '%Y-%m-%d'))\n",
            "                ) <= 1\n",
            "        \n",
            "    - 'Within 10 years' with SQL rule: \n",
            "            abs(date_diff('year',\n",
            "                strptime(\"date_of_birth_l\", '%Y-%m-%d'),\n",
            "                strptime(\"date_of_birth_r\", '%Y-%m-%d'))\n",
            "                ) <= 10\n",
            "        \n",
            "    - 'All other comparisons' with SQL rule: ELSE\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%ag\n",
        "ag_print(bespoke_email_comparison.human_readable_description)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epWoE00iU_Kn",
        "outputId": "71f3321f-9737-41eb-91c2-abd6ad9e2741"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparison 'Exact match vs. Fuzzy Email within damerau_levenshtein thresholds  vs. Fuzzy Username within levenshtein thresholds  vs. Domain-only match vs.anything else' of \"email_address\".\n",
            "Similarity is assessed using the following ComparisonLevels:\n",
            "    - 'Null' with SQL rule: \n",
            "        regexp_extract(\"email_address_l\", '^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+[.][a-zA-Z]{2,}$')\n",
            "     IS NULL OR \n",
            "        regexp_extract(\"email_address_r\", '^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+[.][a-zA-Z]{2,}$')\n",
            "     IS NULL OR\n",
            "                      \n",
            "        regexp_extract(\"email_address_l\", '^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+[.][a-zA-Z]{2,}$')\n",
            "    =='' OR \n",
            "        regexp_extract(\"email_address_r\", '^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+[.][a-zA-Z]{2,}$')\n",
            "     ==''\n",
            "    - 'Exact match email_address' with SQL rule: \"email_address_l\" = \"email_address_r\"\n",
            "    - 'Damerau_levenshtein email_address <= 3' with SQL rule: damerau_levenshtein(\"email_address_l\", \"email_address_r\") <= 3\n",
            "    - 'Damerau_levenshtein Username <= 3' with SQL rule: damerau_levenshtein(\n",
            "        regexp_extract(\"email_address_l\", '^[^@]+')\n",
            "    , \n",
            "        regexp_extract(\"email_address_r\", '^[^@]+')\n",
            "    ) <= 3\n",
            "    - 'Exact match Email Domain' with SQL rule: \n",
            "        regexp_extract(\"email_address_l\", '@([^@]+)$')\n",
            "     = \n",
            "        regexp_extract(\"email_address_r\", '@([^@]+)$')\n",
            "    \n",
            "    - 'All other comparisons' with SQL rule: ELSE\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4q3CzLoROWUn"
      },
      "source": [
        "## Creating a Linker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hzy2HURwOYLw"
      },
      "source": [
        "Using DuckDBLinker."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jx7V-6sbrcAr",
        "outputId": "53195ccc-212a-403f-a225-f09697177482"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/site-packages/splink/blocking_rules_library.py:176: DeprecationWarning: `exact_match_rule` is deprecated; use `block_on`\n",
            "  em_rules = [_exact_match(col) for col in col_names]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%ag\n",
        "from op_splink.duckdb.linker import DuckDBLinker\n",
        "\n",
        "settings = {\n",
        "                \"link_type\": \"link_only\",\n",
        "                \"comparisons\":[\n",
        "                    first_name_comparison,\n",
        "                    last_name_comparison,\n",
        "                    date_difference,\n",
        "                    date_of_birth_comparison,\n",
        "                    bespoke_email_comparison,\n",
        "                ],\n",
        "                \"blocking_rules_to_generate_predictions\": [\n",
        "                    block_on(\"firstname\"),\n",
        "                    block_on(\"lastname\"),\n",
        "                ]\n",
        "}\n",
        "linker = DuckDBLinker([health_link, flight_link], settings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOnqKKSzOoh6"
      },
      "source": [
        "\n",
        "In other words, this setting dictionary says:\n",
        "\n",
        "* We are performing a link_only (the other options are dedupe_only, or link_and_dedupe, which may be used if there are multiple input datasets).\n",
        "* When comparing records, we will use information from the `firstname`, `lastname`, `date`, and `date_of_birth` columns to compute a match score.\n",
        "* The blocking_rules_to_generate_predictions states that we will only check for duplicates amongst records where either the firstname or lastname is identical."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HboTRHhPluu"
      },
      "source": [
        "## Training the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfS7KFRTPnoJ"
      },
      "source": [
        "Now that we have specified our linkage model, we need to estimate the u and m parameters which are used to train the Fellegi Sunter model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8R1iT6HaP32u"
      },
      "source": [
        "The u values are the proportion of records falling into each ComparisonLevel amongst truly non-matching records.\n",
        "\n",
        "We estimate u using the estimate_u_using_random_sampling method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7P2ltJs7rd0d",
        "outputId": "3106a06d-d387-46a5-bed6-0b1e8d575dda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "----- Estimating u probabilities using random sampling -----\n",
            "\n",
            "\n",
            "Estimated u probabilities using random sampling\n",
            "\n",
            "\n",
            "Your model is not yet fully trained. Missing estimates for:\n",
            "    - firstname (no m values are trained).\n",
            "    - lastname (no m values are trained).\n",
            "    - date (no m values are trained).\n",
            "    - date_of_birth (no m values are trained).\n",
            "    - email_address (no m values are trained).\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%ag\n",
        "linker.estimate_u_using_random_sampling(max_pairs=1e6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_HHGyleQYWt"
      },
      "source": [
        "m is the trickiest of the parameters to estimate, because we have to have some idea of what the true matches are.\n",
        "\n",
        "If we have labels, we can directly estimate it. However, if we do not have labelled data, the m parameters can be estimated using an iterative maximum likelihood approach called the **Expectation Maximisation.**\n",
        "\n",
        "Each estimation pass requires the user to configure an estimation blocking rule to reduce the number of record comparisons generated to a manageable level.\n",
        "\n",
        "In our first estimation pass, we block on first_name, meaning we will generate all record comparisons that have first_name exactly equal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wl6g62borlFr",
        "outputId": "9134d5fb-20a0-429b-aa13-4b9993eeda65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/site-packages/splink/blocking_rules_library.py:176: DeprecationWarning: `exact_match_rule` is deprecated; use `block_on`\n",
            "  em_rules = [_exact_match(col) for col in col_names]\n",
            "\n",
            "----- Starting EM training session -----\n",
            "\n",
            "\n",
            "Estimating the m probabilities of the model by blocking on:\n",
            "l.\"firstname\" = r.\"firstname\"\n",
            "\n",
            "Parameter estimates will be made for the following comparison(s):\n",
            "    - lastname\n",
            "    - date\n",
            "    - date_of_birth\n",
            "    - email_address\n",
            "\n",
            "Parameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n",
            "    - firstname\n",
            "\n",
            "\n",
            "\n",
            "EM Converged successfully\n",
            "\n",
            "\n",
            "Your model is not yet fully trained. Missing estimates for:\n",
            "    - firstname (no m values are trained).\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%ag\n",
        "linker.estimate_parameters_using_expectation_maximisation(block_on(\"firstname\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpAFDPBbRgK_"
      },
      "source": [
        "In the second estimation pass, we block on date_of_birth. This allows us to estimate parameters for the first_name and the surname comparisons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTd6SgGRrjRx",
        "outputId": "1ae52238-79b7-4816-eead-9ff960eb3f93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/site-packages/splink/blocking_rules_library.py:176: DeprecationWarning: `exact_match_rule` is deprecated; use `block_on`\n",
            "  em_rules = [_exact_match(col) for col in col_names]\n",
            "\n",
            "----- Starting EM training session -----\n",
            "\n",
            "\n",
            "Estimating the m probabilities of the model by blocking on:\n",
            "l.\"date_of_birth\" = r.\"date_of_birth\"\n",
            "\n",
            "Parameter estimates will be made for the following comparison(s):\n",
            "    - firstname\n",
            "    - lastname\n",
            "    - date\n",
            "    - email_address\n",
            "\n",
            "Parameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n",
            "    - date_of_birth\n",
            "\n",
            "\n",
            "\n",
            "EM Converged successfully\n",
            "\n",
            "\n",
            "Your model is fully trained. All comparisons have at least one estimate for their m and u values\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%ag\n",
        "linker.estimate_parameters_using_expectation_maximisation(block_on(\"date_of_birth\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AawX6ijRzQq"
      },
      "source": [
        "## Predicting Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YjMWTnWSA9j"
      },
      "source": [
        "Now, we need to find the linked dataset. This can be done using the `predict_and_create_linked_df`, which will take a threshold parameter, to extract all the linked records with probability more than the threshold.\n",
        "Getting **ag_timeout error** here ,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "CHMXZxvTrmdC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b140d24-4216-42b0-b1f3-66f918796d2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Error : AG execution timeout.\n"
          ]
        }
      ],
      "source": [
        "%%ag\n",
        "linked_df = linker.predict(0.9)\n",
        "# ag_print(linked_df.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TV_aDoWtSnw-"
      },
      "source": [
        "Counting the number of records within this linked PrivateDataFrame can be done as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5ucnaVvSwj7"
      },
      "source": [
        "### Finding Out Which Flights Should Be Notified"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hx2B-ojrTAjb"
      },
      "source": [
        "To find out which flights should be notified, we can use the following algorithm:\n",
        "\n",
        "* Find all the unique IDs of flight records in the linked dataset. This will include all flights where a COVID-positive passenger was identified in the subsequent 14 days. Let us call this set of unique ids `unique_ids`.\n",
        "* Create a column within the flight dataset, where if `unique_id` of this record belongs to `unique_ids`, the value will be True, else False. This can be done with `isin` method in `op_pandas`. Let us call this column `notify`.\n",
        "* Extract the `flight_number` of all the flights where `notify` is true."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qYy3RWzrzPm"
      },
      "outputs": [],
      "source": [
        "%%ag\n",
        "unique_ids = linked_df['unique_id_r']\n",
        "\n",
        "flights_to_notify_id = flight['unique_id'].isin(unique_ids)\n",
        "flight = flight[['unique_id', 'flight_number']]\n",
        "flight['notify'] = flights_to_notify_id\n",
        "\n",
        "flight = flight.where(flight['notify'] == True)\n",
        "flights_to_notify = flight[['flight_number']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGErT4uJU06X"
      },
      "source": [
        "Now, we will submit the predictions using `submit_predictions` method preloaded within the AG environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJM6ddYKr0VK"
      },
      "outputs": [],
      "source": [
        "%%ag\n",
        "submit_predictions(flights_to_notify)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pA42NZggDCg"
      },
      "source": [
        "Now that we're all done, we use this line to close our work session neatly. It's like turning off the lights when you leave a room â€“ itâ€™s a good habit to wrap things up properly!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtV8oYhNcOG4"
      },
      "outputs": [],
      "source": [
        "session.terminate_session()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}